# -*- coding: utf-8 -*-

import argparse
import tempfile
import traceback
import time
from pathlib import Path
import numpy as np

import gradio as gr
import librosa
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoFeatureExtractor
from transformers.utils import logging

logging.set_verbosity_info()
logger = logging.get_logger(__name__)

class BorealisDemo:
    def __init__(self, device: str = "cuda"):
        self.device = self._get_device(device)
        self.model = None
        self.tokenizer = None
        self.extractor = None
        
        self.default_generation_params = {
            "do_sample": True,
            "top_p": 0.9,
            "top_k": 50,
            "temperature": 0.2,
        }
        
        self.load_model()
        self.model_max_chunk_s = 30
        self.sampling_rate = 16000

    def _get_device(self, requested_device: str) -> str:
        if requested_device == "cuda" and not torch.cuda.is_available():
            print("Warning: CUDA not available. Falling back to CPU.")
            return "cpu"
        return requested_device

    def load_model(self):
        model_name = "Vikhrmodels/Borealis"
        print(f"Loading model '{model_name}' to device '{self.device}'...")
        try:
            self.model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(self.device)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.extractor = AutoFeatureExtractor.from_pretrained(model_name)
            self.model.eval()
            print("‚úÖ Model loaded successfully.")
        except Exception as e:
            print(f"‚ùå Failed to load model. Error: {e}")
            print(traceback.format_exc())
            raise gr.Error("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.")

    def transcribe(self, audio_filepath: str, do_sample: bool, temperature: float, top_p: float, top_k: int):
        if not audio_filepath:
            raise gr.Error("–§–∞–π–ª –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ –∏–ª–∏ –≤–∏–¥–µ–æ.")

        current_generation_params = {
            "max_new_tokens": 350, 
            "do_sample": do_sample,
            "top_p": top_p,
            "top_k": top_k,
            "temperature": temperature,
        }
        
        model_max_chunk_samples = int(self.model_max_chunk_s * self.sampling_rate)

        print(f"Transcribing file: {audio_filepath} with params: {current_generation_params}")
        
        try:
            start_time = time.time()
            waveform, sr = librosa.load(audio_filepath, sr=self.sampling_rate, mono=True)
            
            # 1. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≥—Ä–æ–º–∫–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —Ç–∏—à–∏–Ω—ã
            if np.any(waveform):
                waveform = librosa.util.normalize(waveform)
            audio_duration = len(waveform) / self.sampling_rate

            # 2. –ù–∞—Ö–æ–¥–∏–º —Ä–µ—á–µ–≤—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
            top_db = 30
            speech_intervals = librosa.effects.split(waveform, top_db=top_db)
            
            # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            padding_s = 0.25
            padding_samples = int(padding_s * self.sampling_rate)
            
            final_segments = []
            if speech_intervals.size > 0:
                for start, end in speech_intervals:
                    padded_start = max(0, start - padding_samples)
                    padded_end = min(len(waveform), end + padding_samples)
                    duration = padded_end - padded_start

                    # –ï—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç —Å–∞–º –ø–æ —Å–µ–±–µ –¥–ª–∏–Ω–Ω–µ–µ 30 —Å–µ–∫, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –µ–≥–æ —Ä–µ–∂–µ–º
                    if duration > model_max_chunk_samples:
                        print(f"  > Found a long speech segment ({duration/self.sampling_rate:.2f}s), splitting it...")
                        offset = padded_start
                        while offset < padded_end:
                            segment_end = min(offset + model_max_chunk_samples, padded_end)
                            final_segments.append(waveform[offset:segment_end])
                            offset += model_max_chunk_samples
                    else:
                        final_segments.append(waveform[padded_start:padded_end])
            
            # 4. –ù–æ–≤–∞—è, –Ω–∞–¥–µ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–±–æ—Ä–∫–∏ —á–∞–Ω–∫–æ–≤
            chunks = []
            if final_segments:
                current_chunk = np.array([], dtype=np.float32)
                for segment_waveform in final_segments:
                    if len(current_chunk) + len(segment_waveform) <= model_max_chunk_samples:
                        current_chunk = np.concatenate([current_chunk, segment_waveform])
                    else:
                        if len(current_chunk) > 0:
                            chunks.append(current_chunk)
                        current_chunk = segment_waveform
                if len(current_chunk) > 0:
                    chunks.append(current_chunk)


            # 5. Fallback: –µ—Å–ª–∏ —Ä–µ—á—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –º—É–∑—ã–∫–∞ –∏–ª–∏ —à—É–º), –ø—Ä–æ—Å—Ç–æ —Ä–µ–∂–µ–º –Ω–∞ –∫—É—Å–∫–∏
            if not chunks and len(waveform) > 0:
                print("  > No speech detected, falling back to simple chunking.")
                for i in range(0, len(waveform), model_max_chunk_samples):
                    chunks.append(waveform[i:i + model_max_chunk_samples])

            num_chunks = len(chunks)
            print(f"Audio intelligently split into {num_chunks} chunks.")
            
            full_transcript = []

            for i, audio_chunk in enumerate(chunks):
                progress_log = f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {i + 1}/{num_chunks}..."
                
                yield {
                    "text": "\n\n".join(full_transcript).strip(),
                    "audio_duration": audio_duration,
                    "processing_time": time.time() - start_time,
                    "progress": progress_log
                }
                print(progress_log)

                final_chunk = audio_chunk
                current_len = len(final_chunk)
                if current_len < model_max_chunk_samples:
                    padding = np.zeros(model_max_chunk_samples - current_len, dtype=np.float32)
                    final_chunk = np.concatenate([final_chunk, padding])

                proc = self.extractor(
                    final_chunk,
                    sampling_rate=self.sampling_rate,
                    padding="max_length",
                    max_length=model_max_chunk_samples,
                    return_attention_mask=True,
                    return_tensors="pt",
                )
            
                mel = proc.input_features.to(self.device)
                att_mask = proc.attention_mask.to(self.device)

                with torch.inference_mode():
                    result_list = self.model.generate(mel=mel, att_mask=att_mask, **current_generation_params)
                    
                    text_chunk = ""
                    if isinstance(result_list, list) and len(result_list) > 0 and isinstance(result_list[0], str):
                        text_chunk = result_list[0].strip()
                    
                    print(f"  > Chunk {i + 1} result: '{text_chunk}'")

                    if text_chunk:
                        full_transcript.append(text_chunk)
            
            final_text = "\n\n".join(full_transcript).strip()
            yield {
                "text": final_text,
                "audio_duration": audio_duration,
                "processing_time": time.time() - start_time,
                "progress": f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ {num_chunks}/{num_chunks} —á–∞–Ω–∫–æ–≤."
            }

        except Exception as e:
            print(f"‚ùå Error during transcription: {e}")
            print(traceback.format_exc())
            raise gr.Error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {e}")


def create_ui(demo_instance: BorealisDemo):
    custom_css = """
    .main-header {
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem; border-radius: 15px; margin-bottom: 1.5rem; text-align: center;
        box-shadow: 0 10px 30px rgba(102, 126, 234, 0.2);
    }
    .main-header h1 { color: white; font-size: 2.2rem; font-weight: 700; margin: 0; text-shadow: 0 2px 4px rgba(0,0,0,0.3); }
    .main-header p { color: rgba(255,255,255,0.9); font-size: 1.1rem; margin: 0.5rem 0 0 0; }
    """

    with gr.Blocks(theme=gr.themes.Soft(primary_hue="purple", neutral_hue="slate"), css=custom_css) as interface:
        
        gr.HTML("""
        <div class="main-header">
            <h1>üéôÔ∏è Vikhr Borealis Speech-to-Text</h1>
            <p>–ü–æ—Ä—Ç–∞—Ç–∏–≤–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ—Ç <a href="https://t.me/neuroport" target="_blank" style="color: white;">üëæ –ù–ï–ô–†–û-–°–û–§–¢</a></p>
            <p style="font-size: 0.9rem; opacity: 0.8;">–°–æ–±—Ä–∞–ª <a href="https://t.me/nerual_dreming" target="_blank" style="color: white;">Nerual Dreming</a> - –æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å <a href="https://artgeneration.me/" target="_blank" style="color: white;">ArtGeneration.me</a>, —Ç–µ—Ö–Ω–æ–±–ª–æ–≥–µ—Ä –∏ –Ω–µ–π—Ä–æ-–µ–≤–∞–Ω–≥–µ–ª–∏—Å—Ç.</p>
        </div>
        """)

        with gr.Row():
            with gr.Column(scale=1):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ–π –≤–∫–ª–∞–¥–∫–∏
                selected_tab = gr.State("audio")

                with gr.Tabs() as tabs:
                    with gr.TabItem("üéß –ê—É–¥–∏–æ") as audio_tab:
                        audio_input = gr.Audio(label="–ê—É–¥–∏–æ–≤—Ö–æ–¥", type="filepath", sources=["upload", "microphone"])
                    with gr.TabItem("üé¨ –í–∏–¥–µ–æ") as video_tab:
                        video_input = gr.Video(label="–í–∏–¥–µ–æ–≤—Ö–æ–¥", sources=["upload"])
                
                # –°–ª—É—à–∞—Ç–µ–ª–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏ —Å–º–µ–Ω–µ –≤–∫–ª–∞–¥–∫–∏
                audio_tab.select(lambda: "audio", None, selected_tab, queue=False)
                video_tab.select(lambda: "video", None, selected_tab, queue=False)

                with gr.Accordion("‚öôÔ∏è –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", open=False):
                    params = demo_instance.default_generation_params
                    do_sample_checkbox = gr.Checkbox(label="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (do_sample)", value=params["do_sample"])
                    temperature_slider = gr.Slider(minimum=0.0, maximum=2.0, value=params["temperature"], step=0.1, label="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (temperature)")
                    top_p_slider = gr.Slider(minimum=0.0, maximum=1.0, value=params["top_p"], step=0.05, label="Top-p")
                    top_k_slider = gr.Slider(minimum=0, maximum=200, value=params["top_k"], step=1, label="Top-k")
                
                transcribe_button = gr.Button("üöÄ –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å", variant="primary")

            with gr.Column(scale=2):
                output_textbox = gr.Textbox(label="–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏", lines=15, show_copy_button=True)
                log_output = gr.Textbox(label="–õ–æ–≥", lines=4, interactive=False)
                download_file = gr.File(label="–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç (.txt)", visible=False)

        def transcription_handler(active_tab, audio_file, video_file, do_sample, temperature, top_p, top_k):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–æ–π —Ñ–∞–π–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω–æ–π –≤–∫–ª–∞–¥–∫–∏
            if active_tab == "video":
                filepath = video_file
            else:
                filepath = audio_file

            if filepath is None:
                yield "", "–§–∞–π–ª –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω.", gr.update(visible=False), gr.update(interactive=True)
                return

            yield "", "üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è...", gr.update(visible=False), gr.update(interactive=False)
            
            final_result_data = None
            try:
                for progress_data in demo_instance.transcribe(filepath, do_sample, temperature, top_p, top_k):
                    partial_text = progress_data["text"]
                    progress_log = progress_data["progress"]
                    yield partial_text, f"üîÑ {progress_log}", gr.update(visible=False), gr.update(interactive=False)
                    final_result_data = progress_data

                result_text = final_result_data["text"]
                log_message = (
                    f"‚úÖ –ì–æ—Ç–æ–≤–æ!\n"
                    f"‚è±Ô∏è –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ: {final_result_data['audio_duration']:.2f} —Å–µ–∫.\n"
                    f"‚öôÔ∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {final_result_data['processing_time']:.2f} —Å–µ–∫.\n"
                    f"‚úçÔ∏è –°–∏–º–≤–æ–ª–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ: {len(result_text)}"
                )

                with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as tmp_file:
                    tmp_file.write(result_text)
                    temp_filepath = tmp_file.name
                
                yield result_text, log_message, gr.update(value=temp_filepath, visible=True), gr.update(interactive=True)
            
            except gr.Error as e:
                yield "", f"‚ùå –û—à–∏–±–∫–∞: {e}", gr.update(visible=False), gr.update(interactive=True)
            except Exception as e:
                yield "", f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}", gr.update(visible=False), gr.update(interactive=True)

        
        transcribe_button.click(
            fn=transcription_handler,
            inputs=[selected_tab, audio_input, video_input, do_sample_checkbox, temperature_slider, top_p_slider, top_k_slider],
            outputs=[output_textbox, log_output, download_file, transcribe_button]
        )
        
        gr.Markdown("### üí° **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å**\n1. –í—ã–±–µ—Ä–∏—Ç–µ –≤–∫–ª–∞–¥–∫—É '–ê—É–¥–∏–æ' –∏–ª–∏ '–í–∏–¥–µ–æ'.\n2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –∏–ª–∏ –∑–∞–ø–∏—à–∏—Ç–µ –≥–æ–ª–æ—Å.\n3. (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –†–∞—Å–∫—Ä–æ–π—Ç–µ –º–µ–Ω—é '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏' –∏ –∏–∑–º–µ–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.\n4. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É **üöÄ –†–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å** (–æ–Ω–∞ –∑–∞–±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è –¥–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è —Ä–∞–±–æ—Ç—ã).\n5. –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—è–≤–∏—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –ø–æ–ª–µ —Å–ø—Ä–∞–≤–∞ –≤ –≤–∏–¥–µ –∞–±–∑–∞—Ü–µ–≤.")

    return interface


def main():
    parser = argparse.ArgumentParser(description="Borealis Gradio Demo")
    parser.add_argument("--device", type=str, default="cuda", help="Device to use: 'cuda' or 'cpu'")
    parser.add_argument("--share", action="store_true", help="Create a public link for the demo")
    parser.add_argument("--port", type=int, default=7860, help="Port to run the web server on")
    args = parser.parse_args()

    try:
        demo_instance = BorealisDemo(device=args.device)
        interface = create_ui(demo_instance)
        
        print(f"üöÄ Launching Gradio demo on port {args.port}...")
        interface.queue().launch(
            server_name="0.0.0.0" if args.share else "127.0.0.1",
            server_port=args.port,
            share=args.share,
            show_error=True,
            inbrowser=True
        )
    except gr.Error as e:
        print(f"‚ùå Could not launch the demo due to a critical error: {e}")
    except Exception as e:
        print(f"‚ùå An unexpected error occurred during startup: {e}")
        print(traceback.format_exc())


if __name__ == "__main__":
    main()

